# Available lexical databases #


This is a list of open lexical databases with links to the data.

Note: Clicking on the "(README)" links following the short descriptions will provide more information and access to the data tables.

## Francais ##

### Lexique382 ###

_Lexique382_ est une base de données lexicales du français qui fournit pour ~140000 mots du français: les représentations orthographiques et phonémiques, les lemmes associés, la syllabation, la catégorie grammaticale, le genre et le nombre, les fréquences dans un corpus de livres et dans un corpus de sous-titres de filems, etc. ([README](Lexique382/README-Lexique.md))


### Frantext ###

_Frantext_ fournit la liste de tous les types orthographiques obtenus après tokenization du sous-corpus de Frantext utilisé pour calculer les fréquences "livres"" de Lexique ([README](Frantext/README-Frantext.md)).


### French Lexicon Project ###

The _French Lexicon Project_ (FLP) was inspired from the _English Lexicon Project_ (Balota et al. 2007). It provides visual lexical decision time for about 39000 French words and as many pseudowords. The full data represents 1942000 reactions times from 975 participants. ([README](FrenchLexiconProject/README-FrenchLexiconProject.md))

### Megalex ###

_Megalex_ provides visual and auditory lexical decision times and accuracy rates several thousands of words: Visual lexical decision data are available for 28466 French words and the same number of pseudowords, and auditory lexical decision data are available for 17876 French words and the same number of pseudowords. ([README](Megalex/README-Megalex.md))


### Chronolex ###

_Chronolex_ provides naming times, lexical decision times and progressive demasking scores on most monosyllabic monomorphemic French (about 1500 items). Thirty-seven participants were tested in the naming task, 35 additionnal participants in the lexical decision task and 33 additionnal participants were tested in the progressive demasking task. ([README](Chronolex/README-Chronolex.md))

### Brulex ###

_Brulex_ donne, pour environ 36.000 mots de la langue française, l'orthographe, la prononciation, la classe grammaticale, le genre, le nombre et la fréquence d'usage. Il contient également d'autres informations utiles à la sélection de matériel expérimental (notamment, point d'unicité, comptage des voisins lexicaux, patrons phonologiques, fréquence moyenne des digrammes). ([README](Brulex/README-Brulex.md))


### Gougenheim100 ###

_Gougenheim100_ présente, pour 1064 mots, leur fréquence et leur répartition (nombre de textes dans lesquels ils apparaissent). Le corpus sur lequel, il est basé est un corpus de langue oral basé sur un ensembles d'entretiens avec 275 personnes. C'est donc non seulement un corpus de langue orale mais aussi de langue produite. Le corpus original comprend 163 textes, 312.135 mots et 7.995 lemmes différents ([README](Gougenheim100/README-Gougenheim.md))

### Chacqfam ###

CHACQFAM est une base de données renseignant l’âge d’acquisition estimé et la familiarité de 1225 mots Français ([README](chacqfam/README-Chacqfam.md))


## English (American and British) ##

### SUBTLEXus ###

_SUBTLEXus_ provides two frequency measures based on American movies subtitles (51 million words in total): a) The frequency per million words, called SUBTLEXWF (word form frequency) b) The percentage of films in which a word occurs, called SUBTLEXCD (contextual diversity) ([README](SUBTLEXus/README-SUBTLEXus.md))

### British Lexicon Project ###

The British Lexicon Project (Keuleers et al, 2012) contains lexical decision data for over 28,000 monosyllabic and disyllabic English words. ([README](BritishLexiconProject/README-BritishLexiconProject)).

----

**Similar lists or resources**

- Marc Brysbaert's web site at <http://crr.ugent.be/programs-data>
- English Lexicon Project at <http://elexicon.wustl.edu/>

**Crediting**


* Most databases have associated publications listed in their respective `README** files. They must be cited in any derivative work!


**Accessing**

* Most databases can be queried  at <http://www.lexique.org/shiny/openlexique>


* Most databases are provided in form of `.tsv` or `.csv` files (tab-separated-values or comma-separated-values). These are plain text files which can be easily imported in to R, MATLAB or Python, or even [opened with Excel](https://rievent.zendesk.com/hc/en-us/articles/360000029172-FAQ-How-do-I-open-a-tsv-file-in-Excel-). Check out our [script examples](../scripts/README.md).

* The source tables from most databases can be downloaded from <http://lexique.org/databases.zip> (Warning this file is over 100MB).


**Contributing***

* If you want to contribute, by adding a database, or just sending some corrections, please contact `christophe@pallier.org` and `boris.new@gmail.com**

----

Back to [main page](../README.md)

----

Time-stamp: <2019-04-03 08:36:33 christophe@pallier.org>
